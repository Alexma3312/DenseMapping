# CS6476 Final Project Proposal
## Dense Mapping using Feature Matching and Superpixel Clustering

Mandy Xie, Shicong Ma, Gerry Chen

October 2019

### Problem Statement
One of the fundamental tasks for robot autonomous navigation is to perceive and digitalize the surrounding 3D environment\cite{handa2014benchmark}. To be usable in mobile robot applications, the mapping system needs to fast and densely recover the environment in order to provide sufficient information for navigation.

Unlike other 3d reconstruction methods that reconstructs the environment as a 3D point cloud, we hope to extract surfels \cite{schops2018surfelmeshing, pfister2000surfels, tobor2000rendering} based on extracted superpixels from intensity and depth images and construct a surfel cloud. This approach is introduced by \cite{Wang19icra_surfelDense} which can greatly reduces the memory burden of mapping system when applied to large-scale missions. More importantly, outliers and noise from low-quality depth maps can be reduced based on extracted superpixels.

The **goal** of our project is to reproduce results of Wang et al's, namely implementing superpixel extraction, surfel initialization, and surfel fusion to generate a surfel-based reconstruction given a camera poses from a sparse SLAM implementation.  The **input** to our system will be an RGB-D video stream with accompanying camera poses and the **output** will be a surfel cloud map of the environment, similar to Figures 4b or 8 of the original paper \cite{Wang19icra_surfelDense}.

### Approach

The idea behind dense mapping is to first generate frame related poses, then reconstruct the dense map based on pre-generated poses and surfels.

1. Select a RGB-D dataset \cite{handa2014benchmark,sturm12iros_TUM,Menze2015CVPR_KITTI}

2. Read pose information from the dataset / Use a sparse SLAM system (VINS \cite{qin2018vins}/ORB-SLAM2 \cite{mur2017orb}) to
estimate camera poses

3. **(Suggested implementation)** -- Single frame Superpixels extraction from RGB-D images using a k-means approach adapted from SLIC \cite{achanta2012slic} - IV.D section in \cite{Wang19icra_surfelDense}

4. **(Suggested implementation)** -- Single frame surfel generation based on extracted superpixels. - IV.E section in \cite{Wang19icra_surfelDense}

5. **(Suggested implementation)** --  Surfel fusion and Surfel Cloud update. - IV.G section in \cite{Wang19icra_surfelDense}

% 6. 3D mesh with surfel cloud.


### Experiments and Results

    The intended goal is to produce a surfel map.  As such, our chief datasets will be RGB-D video datasets, preferably with camera pose ground truths, such as the ICL-NUIM \cite{handa2014benchmark}, TUM \cite{sturm12iros_TUM}, and KITTI \cite{Menze2015CVPR_KITTI} datasets.  We will then run our code to generate surfel clouds and quantitatively compare our surfel clouds to those generated by the original paper's implementation \cite{Wang19github} to show we produce similar results.  We will then perform qualitative comparisons of our surfel reconstructions for the different datasets and locations within the datasets. Finally, we will introduce non-idealities in the input data such as camera pose error and depth error to quantitatively and qualitatively evaluate the robustness of our surfel implementation.
    
    Since we aim to replicate the results of the paper, we expect that our surfels should match reasonably well with those generated by the original implementation.  Since we do not consider map deformations, however, we expect that our implementation may be more prone to input data error than the original implementation but the degree of robustness is dependent upon the specifics of the surfel fusion implementation.  Finally, we expect that the resulting surfel cloud should produce qualitatively better results in camera trajectories without loops or backtracking since the robustness of surfel fusion is limited and most accurate when reprojection error is minimal.  A successful project would produce similar results as the original paper although with potentially reduced robustness to camera pose error.
    
    We intend to implement all superpixel and surfel extraction and fusion ourselves as well as some data processing, visualization, and experimental evaluation.  Codes which we do not intend to implement include extraction of camera poses (if not provided by the dataset) and map deformation.
    

%%%%%%%%%%%%%%%%
% References
%%%%%%%%%%%%%%%%
\begin{singlespace}  % use single-line spacing for multi-line text within a single reference
	\setlength\bibitemsep{\baselineskip} 
	% Please add the bibtex to related_work.bib, using Frank's name convention.
	%manually set separation between items in bibliography to double space
	\printbibliography[title={References}]
\end{singlespace}

\end{document}
