<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>index</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="main.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="cs6476-final-project">CS6476 Final Project</h1>
<h3 id="dense-mapping-using-feature-matching-and-superpixel-clustering">Dense Mapping using Feature Matching and Superpixel Clustering</h3>
<p style="color:#808080;margin:16px 0 4px 0;font-size:20px">
Mandy Xie, Shicong Ma, Gerry Chen
</p>
<p style="color:#404040;margin:0 0 16px 0">
Dec 4, 2019
</p>
<hr />
<h2 id="abstract">Abstract</h2>
<!-- One or two sentences on the motivation behind the problem you are solving. One or two sentences describing the approach you took. One or two sentences on the main result you obtained. -->
<p>One of the fundamental tasks for robot autonomous navigation is to perceive and digitalize the surrounding 3D environment <span class="citation" data-cites="handa2014benchmark">[<a href="#ref-handa2014benchmark">1</a>]</span>. We replicate the results of <span class="citation" data-cites="Wang19icra_surfelDense">[<a href="#ref-Wang19icra_surfelDense">2</a>]</span> to produce semi-dense, surfel-based reconstruction using superpixels.</p>
<!-- ### Teaser figure -->
<!-- A figure that conveys the main idea behind the project or the main application being addressed. -->
<figure>
<img src="./results/comparison01.png" alt="Figure: Example semi-dense reconstruction based on a surfel map" /><figcaption><strong>Figure:</strong> Example semi-dense reconstruction based on a surfel map</figcaption>
</figure>
<h2 id="introduction">Introduction</h2>
<!-- Motivation behind the problem you are solving, what applications it has, any brief background on the particular domain you are working in (if not regular RBG photographs), etc. If you are using a new way to solve an existing problem, briefly mention and describe the existing approaches and tell us how your approach is new. -->
<p>One of the fundamental tasks for robot autonomous navigation is to perceive and digitalize the surrounding 3D environment <span class="citation" data-cites="handa2014benchmark">[<a href="#ref-handa2014benchmark">1</a>]</span>. To be usable in mobile robot applications, the mapping system needs to fast and densely recover the environment in order to provide sufficient information for navigation.</p>
<p>Unlike other 3d reconstruction methods that reconstructs the environment as a 3D point cloud, we hope to extract surfels  based on extracted superpixels from intensity and depth images and construct a surfel cloud. This approach is introduced by <span class="citation" data-cites="Wang19icra_surfelDense">[<a href="#ref-Wang19icra_surfelDense">2</a>]</span> which can greatly reduces the memory burden of mapping system when applied to large-scale missions. More importantly, outliers and noise from low-quality depth maps can be reduced based on extracted superpixels.</p>
<p>The <strong>goal</strong> of our project is to reproduce results of Wang et al’s, namely implementing superpixel extraction, surfel initialization, and surfel fusion to generate a surfel-based reconstruction given a camera poses from a sparse SLAM implementation. The <strong>input</strong> to our system is an RGB-D video stream with accompanying camera poses and the <strong>output</strong> is a surfel cloud map of the environment, similar to Figures 4b or 8 of the original paper <span class="citation" data-cites="Wang19icra_surfelDense">[<a href="#ref-Wang19icra_surfelDense">2</a>]</span>.</p>
<h2 id="implementation">Implementation</h2>
<p>The idea behind dense mapping is to first generate frame related poses, then reconstruct the dense map based on pre-generated poses and surfels.</p>
<ol type="1">
<li><p>Select an RGB-D dataset <span class="citation" data-cites="handa2014benchmark sturm12iros_TUM Menze2015CVPR_KITTI">[<a href="#ref-handa2014benchmark">1</a>], [<a href="#ref-sturm12iros_TUM">3</a>], [<a href="#ref-Menze2015CVPR_KITTI">4</a>]</span></p></li>
<li><p>Read pose information from the dataset / Use a sparse SLAM system (VINS <span class="citation" data-cites="qin2018vins">[<a href="#ref-qin2018vins">5</a>]</span>/ORB-SLAM2 <span class="citation" data-cites="mur2017orb">[<a href="#ref-mur2017orb">6</a>]</span>) to estimate camera poses</p></li>
<li><p>Run code from <span class="citation" data-cites="Wang19github">[<a href="#ref-Wang19github">7</a>]</span> directly to confirm functionality and set benchmark/expectations.</p></li>
<li><p><strong>(Implementation)</strong> – Single frame Superpixels extraction from RGB-D images using a k-means approach adapted from SLIC <span class="citation" data-cites="achanta2012slic">[<a href="#ref-achanta2012slic">8</a>]</span> - IV.D section in <span class="citation" data-cites="Wang19icra_surfelDense">[<a href="#ref-Wang19icra_surfelDense">2</a>]</span></p></li>
<li><p><strong>(Implementation)</strong> – Single frame surfel generation based on extracted superpixels. - IV.E section in <span class="citation" data-cites="Wang19icra_surfelDense">[<a href="#ref-Wang19icra_surfelDense">2</a>]</span></p></li>
<li><p><strong>(Implementation)</strong> – Surfel fusion and Surfel Cloud update. - IV.G section in <span class="citation" data-cites="Wang19icra_surfelDense">[<a href="#ref-Wang19icra_surfelDense">2</a>]</span></p></li>
</ol>
<!-- 6. 3D mesh with surfel cloud. -->
<h3 id="dataset">Dataset</h3>
<p>We have started with the <em>kt3</em> sequence of the ICL-NIUM dataset <span class="citation" data-cites="handa2014benchmark">[<a href="#ref-handa2014benchmark">1</a>]</span>. Images and depth maps have been extracted and examples <a href="#sec:dataset">shown below</a>.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">rgb image from ICL-NIUM dataset<span id="sec:dataset"></span></th>
<th style="text-align: center;">depth image from ICL-NIUM dataset</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><img align="center" src="./results/superpixels/icl_rgb0.png" width="500"/></td>
<td style="text-align: center;"><img align="center" src="./results/superpixels/icl_depth0.png" width="500"/></td>
</tr>
</tbody>
</table>
<p><br /></p>
<h3 id="run-existing-code">Run Existing Code</h3>
<p>The code written for the paper was run to ensure that the results could be reproduced. Below are some results of running the code for dense reconstruction on images from the KITTI dataset <span class="citation" data-cites="Menze2015CVPR_KITTI">[<a href="#ref-Menze2015CVPR_KITTI">4</a>]</span>. We showed that it can indeed produce dense reconstructions.</p>
<p><img src="./results/kitti/kitti.png" alt="kitti" style="width:45.0%" /> <img src="./results/kitti/kitti_front.png" alt="kitti" style="width:45.0%" /></p>
<p><img src="./results/kitti/kitti_side.png" alt="kitti" style="width:45.0%" /> <img src="./results/kitti/kitti_top.png" alt="kitti" style="width:45.0%" /></p>
<h3 id="superpixel-extraction">Superpixel Extraction</h3>
<p>We have completed single-frame superpixel generation. The results are <a href="#fig:sp_extr">shown below</a>.</p>
<p><img src="./results/superpixels/superpixels_rgb.gif" alt="superpixel annotation on RGB image" id="fig:sp_extr" style="width:45.0%" /> <img src="./results/superpixels/superpixels_depth.gif" alt="superpixel annotation on depth image" style="width:45.0%" /></p>
<p>We follow the standard implementation as described in the paper:</p>
<ol type="1">
<li>Initialize superpixel seeds - Superpixel seeds are initialized on a grid of predefined size<br />
</li>
<li>Update superpixels
<ol type="1">
<li>Pixels are assigned to their nearest superpixel<br />
</li>
<li>Superpixel properties (x, y, size, intensity, depth) are updated accordingly.</li>
</ol></li>
</ol>
<p>The number of times that step 2 is repeated depends on the image. For example, the ICL-NIUM dataset image requires only 10 iterations or so to stabilize, but the KITTI dataset image requires roughly 25 to stabilize. One metric that can potentially be used in the future as a stopping criteria is the sum of distances traveled by the superpixels from one iteration to the next. The superpixel iteration is complete when they sufficiently small changes occur from one iteration to the next.</p>
<h3 id="surfel-generation-and-fusion">Surfel Generation and Fusion</h3>
<!-- norm calculation -->
<!-- We are in the process of calculating the norm which is needed for surfel
generation.  We expect to complete this very soon. -->
<p>Surfels are modeled with the superpixels extracted from intensity and depth images in the following method as described in the paper:</p>
<ol type="1">
<li>Surfel Initialization:<br />
Initialize superpixel cluster that has enough assigned seeds with a set of reasonable initial value.<br />
</li>
<li>Surfel Fusion:<br />
Fuse extrated local surfels with newly initalized surfels if they have similar depth and normals. Transform fused local surfels into the global frame, and remove those are updated less than 5 times.</li>
</ol>
<p>For the surfel initialization, we transform each superpixel into a surfel according to the correspondence equations given in <span class="citation" data-cites="Wang19icra_surfelDense">[<a href="#ref-Wang19icra_surfelDense">2</a>]</span>. An example of a single frame with each superpixel processed in this manner is <a href="#fig:multiframesurfel">shown below</a>.</p>
<figure>
<img src="./results/frames01.png" alt="Figure: Single frame surfel reconstruction" id="fig:singleframesurfel" /><figcaption><strong>Figure:</strong> Single frame surfel reconstruction</figcaption>
</figure>
<p>To process additional frames, each new surfel from a superpixel in the frame to be added must first be checked amongst all existing surfels to see if it is similar enough to be fused. If not, then a new surfel is initialized. An example of a cloud consisting of a few frames containing fused surfels is <a href="#fig:multiframesurfel">shown below</a>.</p>
<figure>
<img src="./results/comparison01.png" alt="Figure: Multi-frame surfel fusion result" id="fig:multiframesurfel" /><figcaption><strong>Figure:</strong> Multi-frame surfel fusion result</figcaption>
</figure>
<p>We can see that the reconstruction is much better filled in (denser) and also has better detail. For example, the filing cabinet achieves much better resolution when putting multiple frames together. This is partly because multiple surfels can occupy the same location in space if they have different normal directions.</p>
<p>We have now recreated the dense reconstruction surfel cloud results from <span class="citation" data-cites="Wang19icra_surfelDense">[<a href="#ref-Wang19icra_surfelDense">2</a>]</span> that we sought out to achieve.</p>
<h2 id="experiments-and-results">Experiments and results</h2>
<!-- Provide details about the experimental set up (number of images/videos, number of datasets you experimented with, train/test split if you used machine learning algorithms, etc.). Describe the evaluation metrics you used to evaluate how well your approach is working. Include clear figures and tables, as well as illustrative qualitative examples if appropriate. Be sure to include obvious baselines to see if your approach is doing better than a naive approach (e.g. for classification accuracy, how well would a classifier do that made random decisions?). Also discuss any parameters of your algorithms, and tell us how you set the values of those parameters. You can also show us how the performance varies as you change those parameter values. Be sure to discuss any trends you see in your results, and explain why these trends make sense. Are the results as expected? Why? -->
<p>A number of parameters can be tuned for surfel generation and a few will be discussed:</p>
<ol type="1">
<li>K-means iterations for superpixel extraction</li>
<li>Number of frames used in surfel cloud generation</li>
<li>Superpixel size used in initialization of superpixel seeds</li>
<li>Outlier removal criteria</li>
</ol>
<p>We then finish with a qualitative description of the resulting surfel clouds.</p>
<h3 id="k-means-iterations">K-means Iterations</h3>
<p>As mentioned earlier and shown in both the <a href="#fig:sp_extr">figure above</a> and the <a href="#fig:sp_iter">figure below</a>, the process of extracting superpixels is an iterative one based on k-means. This means that the number of iterations to run k-means affects how closely the superpixels will converge to the optimal superpixel assignments. The animated figures illustrate how the superpixel assignments change as more iterations are run. We found that, for the ICL-NIUM dataset, roughly 10 iterations were sufficient while roughly 25 were required for the KITTI dataset. We follow the lead of the reference paper <span class="citation" data-cites="Wang19icra_surfelDense">[<a href="#ref-Wang19icra_surfelDense">2</a>]</span> and consider the number of iterations a human-tuned parameter instead of setting a convergence stopping criterion.</p>
<p><img src="./results/superpixels/kitti_superpixels_rgb.gif" alt="superpixel annotation on RGB image" id="fig:sp_iter" style="width:100.0%" /> <img src="./results/superpixels/kitti_superpixels_depth.gif" alt="superpixel annotation on depth image" style="width:100.0%" /> <strong>Figure:</strong> Superpixel extraction over multiple iterations</p>
<h3 id="number-of-frames">Number of Frames</h3>
<p>The number of frames to use to generate a surfel cloud significantly affects the result. This is because the pose estimate that we read in was not completely accurate, so errors accrue with more frames causing inconsistencies in the surfel cloud. At the same time, too few frames results in sparser clouds with more gaps. <a href="#fig:frames">Shown below</a> are examples of surfel clouds generated with 1, 3, 25, and 50 frames.</p>
<figure>
<img src="./results/frames.gif" alt="Figure: Effect of the number of frames used on surfel cloud result" id="fig:frames" /><figcaption><strong>Figure:</strong> Effect of the number of frames used on surfel cloud result</figcaption>
</figure>
<h3 id="superpixel-size">Superpixel Size</h3>
<p>The generated surfels result varies when we change the parameters, such as the size of superpixels and the size of surfels. The number of superpixels in our implementation does not change during the k-means process so the size of initialized superpixels affects the general size scale of the final superpixels as well. Similarly, surfel size is dependent upon superpixel size because surfels are initialized from superpixels, so the superpixel initialization density also affects the final sizes of the surfels. <a href="#fig:size">Shown below</a> are examples of surfel clouds generated with initialization superpixel sizes of 50x50, 25x25, 12x12, and 9x9. We see that too large superpixels lose detail while too small superpixels become sparse.</p>
<figure>
<img src="./results/size.gif" alt="Figure: Effect of superpixel initialization size on surfel cloud" id="fig:size" /><figcaption><strong>Figure:</strong> Effect of superpixel initialization size on surfel cloud</figcaption>
</figure>
<h3 id="outlier-removal">Outlier Removal</h3>
<p>Some surfels are poorly conditioned due to factors such as oblique viewpoint, small superpixel parent, only being visible in few frames, distance to camera, and other factors. Several checks exist in our code to eliminate obvious outliers. One example is removing surfels which don’t appear in many frames. Surfels which appear in multiple frames get “fused” and we keep track of how many times a given surfel has been fused. The <a href="#fig:outlier">animation below</a> compares a raw surfel cloud and one which removes surfels fused less than 3 times. We notice that including these “outlier” surfels generates a more complete cloud, but at the expense of extra noise. For example, there is a cluster of surfels to the right of the filing cabinet which are not oriented correctly.</p>
<figure>
<img src="./results/outliers.gif" alt="Figure: Effect of outlier removal on surfel cloud" id="fig:outlier" /><figcaption><strong>Figure:</strong> Effect of outlier removal on surfel cloud</figcaption>
</figure>
<h3 id="qualitative-results">Qualitative Results</h3>
<!-- Show several visual examples of inputs/outputs of your system (success cases and failures) that help us better understand your approach. -->
<p>The superpixel-segmented <a href="#fig:kitti_sp">images below</a> from the KITTI <span class="citation" data-cites="Menze2015CVPR_KITTI">[<a href="#ref-Menze2015CVPR_KITTI">4</a>]</span> dataset demonstrate that the superpixels are indeed segmenting properly as they tend to “hug” similarly colored/depthed regions.</p>
<p><img src="./results/superpixels/kitti_superpixels_rgb.png" alt="superpixel annotation on RGB image" id="fig:kitti_sp" style="width:100.0%" /> <img src="./results/superpixels/kitti_superpixels_depth.png" alt="superpixel annotation on depth image" style="width:100.0%" /></p>
<p>Converting the superpixels into surfels appears correct based on:</p>
<ol type="1">
<li>The locations of the surfels clearly traces out the room shape (see <a href="#fig:dataset">reference image</a>)</li>
<li>The orientations of the surfels are aligned with their neighbors to form planar surfaces</li>
<li>The orientations of the surfels match the room shape</li>
<li>The orientations of the surfels near corners are slightly “curved” indicating fusing and averaging is working properly</li>
<li>Overlapping and non-redundancy of surfels from multiple frames indicates fusion is working properly to only add new surfels when they are sufficiently different than the nearby surfels.</li>
</ol>
<p>We can observe these qualitatively from the <a href="#fig:dataset">rgb image</a> and <a href="#fig:qualitativeSurfel">example surfel cloud below</a>.</p>
<figure>
<img src="./results/snapshot00.png" alt="Figure: Surfel cloud from 25 frames which matches expectations" id="fig:qualitativeSurfel" /><figcaption><strong>Figure:</strong> Surfel cloud from 25 frames which matches expectations</figcaption>
</figure>
<h2 id="conclusion-and-future-work">Conclusion and Future Work</h2>
<!-- Conclusion would likely make the same points as the abstract. Discuss any future ideas you have to make your approach better. -->
<p>We successfully recreated the results of <span class="citation" data-cites="Wang19icra_surfelDense">[<a href="#ref-Wang19icra_surfelDense">2</a>]</span> by creating a surfel cloud given RGBD images and camera poses. Furthermore, we investigated and reported the effects of various parameters on the resulting surfel clouds and discussed qualitative results from our reconstructions.</p>
<p>During this project, several difficulties were faces, such as</p>
<ul>
<li>Debugging conversions between camera projections, coordinate frames, and data types</li>
<li>Robustly calculating the normal direction for a surfel which requires applying an outlier-robust method of combining approximate normal directions for each pixel contained in a superpixel</li>
<li>Achieving reasonable performance speed using matrix manipulation instead of for loop and multi-threads in C++ to reduce computationl cost</li>
<li>Understanding the meaning of different values in a surfel vector</li>
</ul>
<p>Future directions include</p>
<ul>
<li>Intelligent k-means superpixel stopping criterion</li>
<li>Faster speed in superpixel extraction</li>
<li>Better pose estimation that can also feed-back pose corrections based on surfel matches</li>
<li>Better surfel outlier rejection</li>
</ul>
<h2 id="references">References</h2>
<!-- List out all the references you have used for your work -->
<div id="refs" class="references">
<div id="ref-handa2014benchmark">
<p>[1] A. Handa, T. Whelan, J. McDonald, and A. J. Davison, “A benchmark for rgb-d visual odometry, 3D reconstruction and slam,” in <em>2014 ieee international conference on robotics and automation (icra)</em>, 2014, pp. 1524–1531.</p>
</div>
<div id="ref-Wang19icra_surfelDense">
<p>[2] K. Wang, F. Gao, and S. Shen, “Real-time scalable dense surfel mapping,” in <em>2019 international conference on robotics and automation (icra)</em>, 2019, pp. 6919–6925.</p>
</div>
<div id="ref-sturm12iros_TUM">
<p>[3] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers, “A benchmark for the evaluation of rgb-d slam systems,” in <em>Proc. Of the international conference on intelligent robot systems (iros)</em>, 2012.</p>
</div>
<div id="ref-Menze2015CVPR_KITTI">
<p>[4] M. Menze and A. Geiger, “Object scene flow for autonomous vehicles,” in <em>Conference on computer vision and pattern recognition (cvpr)</em>, 2015.</p>
</div>
<div id="ref-qin2018vins">
<p>[5] T. Qin, P. Li, and S. Shen, “Vins-mono: A robust and versatile monocular visual-inertial state estimator,” <em>IEEE Transactions on Robotics</em>, vol. 34, no. 4, pp. 1004–1020, 2018.</p>
</div>
<div id="ref-mur2017orb">
<p>[6] R. Mur-Artal and J. D. Tardós, “Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras,” <em>IEEE Transactions on Robotics</em>, vol. 33, no. 5, pp. 1255–1262, 2017.</p>
</div>
<div id="ref-Wang19github">
<p>[7] https://github.com/HKUST-Aerial-Robotics/DenseSurfelMapping.</p>
</div>
<div id="ref-achanta2012slic">
<p>[8] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk, “SLIC superpixels compared to state-of-the-art superpixel methods,” <em>IEEE transactions on pattern analysis and machine intelligence</em>, vol. 34, no. 11, pp. 2274–2282, 2012.</p>
</div>
</div>
</body>
</html>
