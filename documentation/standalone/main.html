<?xml version="1.0" encoding="iso-8859-1" ?> 
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" 
"http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd" > 
<html xmlns="http://www.w3.org/1999/xhtml"  
> 
<head> <title>CS6476 Final Project Proposal</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" /> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)" /> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)" /> 
<!-- html,xhtml,mathml --> 
<meta name="src" content="main.tex" /> 
<meta name="date" content="2019-10-08 15:00:00" /> 
<link rel="stylesheet" type="text/css" href="main.css" /> 
</head><body 
>
   <div class="maketitle">



<h2 class="titleHead">CS6476 Final Project Proposal<br />Dense Mapping using Feature Matching and Superpixel Clustering</h2>
<div class="author" ><span 
class="cmr-12">Mandy Xie, Shicong Ma, Gerry Chen</span></div><br />
<div class="date" ><span 
class="cmr-12">October 2019</span></div>
   </div>
   <h3 class="sectionHead"><span class="titlemark">1   </span> <a 
 id="x1-10001"></a>Problem Statement</h3>
<p>One of the fundamental tasks for robot autonomous navigation is to perceive and digitalize the surrounding 3D
environment[1]. To be usable in mobile robot applications, the mapping system needs to fast and densely recover the
environment in order to provide sufficient information for navigation.
</p> 
<p>   Unlike other 3d reconstruction methods that reconstructs the environment as a 3D point cloud, we hope to
extract surfels [2, 3, 4] based on extracted superpixels from intensity and depth images and construct a surfel cloud.
This approach is introduced by [5] which can greatly reduces the memory burden of mapping system when applied
to large-scale missions. More importantly, outliers and noise from low-quality depth maps can be reduced based on
extracted superpixels.
</p> 
<p>   The <b><span 
class="cmbx-10">goal</span></b> of our project is to reproduce results of Wang et al&#8217;s, namely implementing superpixel extraction,
surfel initialization, and surfel fusion to generate a surfel-based reconstruction given a camera poses from a sparse
SLAM implementation. The <b><span 
class="cmbx-10">input</span></b> to our system will be an RGB-D video stream with accompanying camera poses
and the <b><span 
class="cmbx-10">output</span></b> will be a surfel cloud map of the environment, similar to Figures 4b or 8 of the original paper
[5].
</p> 
<p>
   </p> 

   <h3 class="sectionHead"><span class="titlemark">2   </span> <a 
 id="x1-20002"></a>Approach</h3>
<p>The idea behind dense mapping is to first generate frame related poses, then reconstruct the dense map based on
pre-generated poses and surfels.
</p> 
<p>
     </p> 
<ol  class="enumerate1" >
     <li 
  class="enumerate" id="x1-2002x1">Select a RGB-D dataset [1, 6, 7]
     </li>
     <li 
  class="enumerate" id="x1-2004x2">Read pose information from the dataset / Use a sparse SLAM system (VINS [8]/ORB-SLAM2 [9]) to
     estimate camera poses
     </li>
     <li 
  class="enumerate" id="x1-2006x3"><b><span 
class="cmbx-10">(Suggested implementation)</span></b> &#8211; Single frame Superpixels extraction from RGB-D images using a
     k-means approach adapted from SLIC [10] - IV.D section in [5]
     </li>
     <li 
  class="enumerate" id="x1-2008x4"><b><span 
class="cmbx-10">(Suggested implementation)</span></b> &#8211; Single frame surfel generation based on extracted superpixels. - IV.E
     section in [5]

     </li>
     <li 
  class="enumerate" id="x1-2010x5"><b><span 
class="cmbx-10">(Suggested implementation)</span></b> &#8211; Surfel fusion and Surfel Cloud update. - IV.G section in [5]
     </li></ol>
<p>
   </p> 

   <h3 class="sectionHead"><span class="titlemark">3   </span> <a 
 id="x1-30003"></a>Experiments and Results</h3>
<p>The intended goal is to produce a surfel map. As such, our chief datasets will be RGB-D video datasets,
preferably with camera pose ground truths, such as the ICL-NUIM [1], TUM [6], and KITTI [7] datasets.
We will then run our code to generate surfel clouds and quantitatively compare our surfel clouds to
those generated by the original paper&#8217;s implementation [11] to show we produce similar results. We
will then perform qualitative comparisons of our surfel reconstructions for the different datasets and
locations within the datasets. Finally, we will introduce non-idealities in the input data such as camera
pose error and depth error to quantitatively and qualitatively evaluate the robustness of our surfel
implementation.
</p> 
<p>   Since we aim to replicate the results of the paper, we expect that our surfels should match reasonably well with
those generated by the original implementation. Since we do not consider map deformations, however, we expect
that our implementation may be more prone to input data error than the original implementation but the degree of
robustness is dependent upon the specifics of the surfel fusion implementation. Finally, we expect that the resulting
surfel cloud should produce qualitatively better results in camera trajectories without loops or backtracking since
the robustness of surfel fusion is limited and most accurate when reprojection error is minimal. A successful project
would produce similar results as the original paper although with potentially reduced robustness to camera pose
error.
</p> 
<p>   We intend to implement all superpixel and surfel extraction and fusion ourselves as well as some data processing,
visualization, and experimental evaluation. Codes which we do not intend to implement include extraction of camera
poses (if not provided by the dataset) and map deformation.
</p> 
<p>
   </p> 

   <h3 class="sectionHead"><a 
 id="x1-40003"></a>References</h3>
<p>
     </p> 
<dl class="thebibliography"><dt id="X0-handa2014benchmark" class="thebibliography">
 [1]  </dt><dd 
id="bib-1" class="thebibliography">
     <p><a id="page.4"></a><a 
\relax  id="X0-" ></a>A. Handa, T. Whelan, J. McDonald, and A. J. Davison, &#8220;A benchmark for rgb-d visual odometry, 3d
     reconstruction and slam,&#8221; in <span 
class="cmti-10">2014 IEEE international conference on Robotics and automation (ICRA)</span>,
     IEEE, 2014, pp. 1524&#8211;1531.
     </p> 
</dd><dt id="X0-schops2018surfelmeshing" class="thebibliography">
 [2]  </dt><dd 
id="bib-2" class="thebibliography">
     <p>T. Schöps, T. Sattler, and M. Pollefeys, &#8220;Surfelmeshing: Online surfel-based mesh reconstruction,&#8221;
     <span 
class="cmti-10">ArXiv preprint arXiv:1810.00729</span>, 2018.
     </p> 
</dd><dt id="X0-pfister2000surfels" class="thebibliography">
 [3]  </dt><dd 
id="bib-3" class="thebibliography">
     <p>H. Pfister, M. Zwicker, J. Van Baar, and M. Gross, &#8220;Surfels: Surface elements as rendering primitives,&#8221;
     in <span 
class="cmti-10">Proceedings of the 27th annual conference on Computer graphics and interactive techniques</span>, ACM
     Press/Addison-Wesley Publishing Co., 2000, pp. 335&#8211;342.

     </p> 
</dd><dt id="X0-tobor2000rendering" class="thebibliography">
 [4]  </dt><dd 
id="bib-4" class="thebibliography">
     <p>I. Tobor, C. Schlick, and L. Grisoni, &#8220;Rendering by surfels,&#8221; in <span 
class="cmti-10">Proceedings of Graphicon</span>, vol.&#x00A0;3, 2000,
     p. 25.
     </p> 
</dd><dt id="X0-Wang19icra_surfelDense" class="thebibliography">
 [5]  </dt><dd 
id="bib-5" class="thebibliography">
     <p>K. Wang, F. Gao, and S. Shen, &#8220;Real-time scalable dense surfel mapping,&#8221; in <span 
class="cmti-10">2019 International</span>
     <span 
class="cmti-10">Conference on Robotics and Automation (ICRA)</span>, IEEE, 2019, pp. 6919&#8211;6925.
     </p> 
</dd><dt id="X0-sturm12iros_TUM" class="thebibliography">
 [6]  </dt><dd 
id="bib-6" class="thebibliography">
     <p>J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers, &#8220;A benchmark for the evaluation of
     rgb-d slam systems,&#8221; in <span 
class="cmti-10">Proc. of the International Conference on Intelligent Robot Systems (IROS)</span>,
     2012.
     </p> 
</dd><dt id="X0-Menze2015CVPR_KITTI" class="thebibliography">
 [7]  </dt><dd 
id="bib-7" class="thebibliography">
     <p>M. Menze and A. Geiger, &#8220;Object scene flow for autonomous vehicles,&#8221; in <span 
class="cmti-10">Conference on Computer</span>
     <span 
class="cmti-10">Vision and Pattern Recognition (CVPR)</span>, 2015.
     </p> 
</dd><dt id="X0-qin2018vins" class="thebibliography">
 [8]  </dt><dd 
id="bib-8" class="thebibliography">
     <p>T.  Qin,  P.  Li,  and  S.  Shen,  &#8220;Vins-mono:  A  robust  and  versatile  monocular  visual-inertial  state
     estimator,&#8221; <span 
class="cmti-10">IEEE Transactions on Robotics</span>, vol. 34, no. 4, pp. 1004&#8211;1020, 2018.
     </p> 
</dd><dt id="X0-mur2017orb" class="thebibliography">
 [9]  </dt><dd 
id="bib-9" class="thebibliography">
     <p>R. Mur-Artal and J. D. Tardós, &#8220;Orb-slam2: An open-source slam system for monocular, stereo, and
     rgb-d cameras,&#8221; <span 
class="cmti-10">IEEE Transactions on Robotics</span>, vol. 33, no. 5, pp. 1255&#8211;1262, 2017.
     </p> 
</dd><dt id="X0-achanta2012slic" class="thebibliography">
[10]  </dt><dd 
id="bib-10" class="thebibliography">
     <p>R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk, &#8220;Slic superpixels compared to
     state-of-the-art superpixel methods,&#8221; <span 
class="cmti-10">IEEE transactions on pattern analysis and machine intelligence</span>,
     vol. 34, no. 11, pp. 2274&#8211;2282, 2012.
     </p> 
</dd><dt id="X0-Wang19github" class="thebibliography">
[11]  </dt><dd 
id="bib-11" class="thebibliography">
     <p>https://github.com/HKUST-Aerial-Robotics/DenseSurfelMapping.</p> 
</dd></dl>
    
</body></html> 



